{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOMSA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc29_rfTL00B"
      },
      "source": [
        "Organizing our Data, Twitter Sentiment Analysis Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAkZu7VrLirQ",
        "outputId": "ab81bae4-0331-4692-98c1-663457550424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extracing the Data from a CSV, Pulling form Columns 1 and 3\n",
        "training = np.genfromtxt('/content/SentimentAnalysisDataset2.csv', delimiter=',', skip_header = 1, usecols = (1, 3), dtype = None)\n",
        "\n",
        "# Create our Training Data\n",
        "train_x = [x[1] for x in training]\n",
        "\n",
        "# Index All the Sentiment Labels\n",
        "train_y = np.asarray([x[0] for x in training])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfivJjOmMnJ3"
      },
      "source": [
        "Use Keras to Preprocess the Data and Make it Machine-Friendly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wThkJrlHMsCH"
      },
      "source": [
        "import json\n",
        "import keras\n",
        "import keras.preprocessing.text as kpt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Tokenizer Will Only Look at the 3000 Most Popular Words\n",
        "# Low Frequency Words Don't Offer Much Imput, or Could be Typos\n",
        "# It Also Saves Time Since We're Using a Boat Load of Data\n",
        "max_words = 3000\n",
        "\n",
        "# Create Tokenizer\n",
        "tokenizer = Tokenizer(num_words = max_words)\n",
        "\n",
        "# Feed Data into the Tokenizer\n",
        "tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "# Tokenizers Come with a Convenient List of Words and IDs :) !\n",
        "dictionary = tokenizer.word_index\n",
        "\n",
        "# Saving Dictionary.json for Later Use\n",
        "with open('dictionary.json', 'w') as dictionary_file:\n",
        "  json.dump(dictionary, dictionary_file)\n",
        "\n",
        "def convert_text_to_index_array(text):\n",
        "  # Makes All Texts the Same Length as the Longest in the Set\n",
        "  return [dictionary[word] for word in\n",
        "          kpt.text_to_word_sequence(text)]\n",
        "        \n",
        "allWordIndices = []\n",
        "# Change Each Token to its ID in the Tokenizers word_index\n",
        "for text in train_x:\n",
        "  wordIndices = convert_text_to_index_array(text)\n",
        "  allWordIndices.append(wordIndices)\n",
        "\n",
        "# Cast List of All Tweets Converted to index Array as Array\n",
        "allWordIndices = np.asarray(allWordIndices)\n",
        "\n",
        "#Create One-Hot Matrices Out of Indexed Twets\n",
        "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode = 'binary')\n",
        "\n",
        "# Labels as Categories\n",
        "train_y = keras.utils.to_categorical(train_y, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo-OLLjNpbI7"
      },
      "source": [
        "Making the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IRDukA5pZaD"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape = (max_words,), activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation = 'sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation = 'softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYx4i9YrqjZT"
      },
      "source": [
        "Compile the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ompNOTckqhdO"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zkMImGLqBxa"
      },
      "source": [
        "Training the Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZTiSvYfqF5F",
        "outputId": "dc644e88-4c65-4fd7-b516-e5ac43e85c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "model.fit(train_x, train_y,\n",
        "          batch_size = 32,\n",
        "          epochs = 5,\n",
        "          verbose = 1,\n",
        "          validation_split = 0.1,\n",
        "          shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "19688/19688 [==============================] - 83s 4ms/step - loss: 0.5171 - accuracy: 0.7441 - val_loss: 0.4974 - val_accuracy: 0.7608\n",
            "Epoch 2/5\n",
            "19688/19688 [==============================] - 89s 5ms/step - loss: 0.4886 - accuracy: 0.7642 - val_loss: 0.4850 - val_accuracy: 0.7681\n",
            "Epoch 3/5\n",
            "19688/19688 [==============================] - 98s 5ms/step - loss: 0.4717 - accuracy: 0.7753 - val_loss: 0.4845 - val_accuracy: 0.7680\n",
            "Epoch 4/5\n",
            "19688/19688 [==============================] - 97s 5ms/step - loss: 0.4593 - accuracy: 0.7835 - val_loss: 0.4839 - val_accuracy: 0.7678\n",
            "Epoch 5/5\n",
            "19688/19688 [==============================] - 96s 5ms/step - loss: 0.4490 - accuracy: 0.7903 - val_loss: 0.4842 - val_accuracy: 0.7687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7d8d9a4d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rmNGeZm_PbA"
      },
      "source": [
        "Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWG1MTHu_M_N"
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "model.save_weights('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW4RHQCArJVY"
      },
      "source": [
        "Using the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX6kAhazrLpU",
        "outputId": "694fe79b-20a1-46ae-b7f4-062306ebfaf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import keras\n",
        "import keras.preprocessing.text as kpt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import model_from_json\n",
        "\n",
        "# Utilizing Tokenizer\n",
        "tokenizer = Tokenizer(num_words = 3000)\n",
        "\n",
        "# Labels for Printing\n",
        "labels = ['Negative', 'Positive']\n",
        "\n",
        "# Accessing our Dictionary\n",
        "with open('dictionary.json', 'r') as dictionary_file:\n",
        "  dictionary = json.load(dictionary_file)\n",
        "\n",
        "# Checking to Make Sure Words were in Training Corpus\n",
        "# Before Converting into a Matrix\n",
        "def convert_text_to_index_array(text):\n",
        "  words = kpt.text_to_word_sequence(text)\n",
        "  wordIndices = []\n",
        "  for word in words:\n",
        "    if word in dictionary:\n",
        "      wordIndices.append(dictionary[word])\n",
        "    else:\n",
        "      pass\n",
        "      #print(\"'%s' is not in the Training Corpus, Ignoring.\" %(word))\n",
        "  return wordIndices\n",
        "\n",
        "# Read our Saved Model Structure\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "# Creating Model from that Model\n",
        "model = model_from_json(loaded_model_json)\n",
        "# Adding Weight to the Nodes\n",
        "model.load_weights('model.h5')\n",
        "\n",
        "# THIS IS WHERE YOU INPUT THE FILE OR SENTENCE\n",
        "\n",
        "str = open('/content/RedditTest3.json', 'r').read()\n",
        "# evalSentence = ('You are amazing')\n",
        "\n",
        "\n",
        "testArr = convert_text_to_index_array(str)#evalSentence)\n",
        "input = tokenizer.sequences_to_matrix([testArr], mode = 'binary')\n",
        "\n",
        "    # Predict if Positive or Negative\n",
        "pred = model.predict(input)\n",
        "\n",
        "print(\"%s Sentiment and %f%% Confidence\" % (labels[np.argmax(pred)], pred[0][np.argmax(pred)] * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7d5cd9f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Positive Sentiment and 65.915877% Confidence\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}